\section{Introduction}

% What is the problem?
% * Data is getting more complicated
% * Analytics are getting more complicated
% * 

The amount of data generated by the world is increasing at an accelerating rate. Data is also becoming more accessible to the public domain, which has caused an emergence of applications in data science that must run on consumer-grade hardware. Streaming sensor data, graph social networks, relational tables of product information, and tensors of climate science image data are few examples of datasets that can scale to the point where they can no longer be managed by a single machine. As a result, distributed shared-nothing data-parallel systems have become the norm for data intensive computing. These systems are able to scale against increasing problem sizes by partitioning data and parallelising computation across machines. Distributed systems programming, in its barest form, is however known to be notoriously difficult.

Without proper abstraction, application developers must manage problems such as fault tolerance and coordination while considering tradeoffs in security and efficiency. To this end, distributed systems leverage high-level DSLs in the form of query languages, frameworks, and libraries, which are more friendly towards end-users. DSLs allow developers to focus on domain-specific problems, such as the development of algorithms, and to disregard engineering-related issues. Not only do DSLs lend themselves to improved ease of use, but also optimisation potential. DSLs in the form of intermediate languages have been adopted by multiple systems both as a solution to enable reuse by breaking the dependence between the program specification and its execution, and to enable target-independent optimisation.


% The need for real-time analytics to uncover the deeper meaning of live data has become more prominent in recent years. In both academia and industry, large scale machine learning systems are a prime focus of research. Such systems have been optimised to train opaque machine learning models of many parameters against large amounts of input data. As an example, GPT-3 by OpenAI is the hallmark of general pre-trained models for language prediction. GPT-3 holds a capacity of 175 billion parameters and was trained on a dataset which effectively amounts to the entire Internet.

% While the models and the supporting technologies behind them have been fine-tuned for accuracy and performance, less effort has been dedicated to their integration with \textit{real-time analytics}. For this reason, the leap from prototyping a machine learning model to deploying it as an online service for mission-critical decision making requires both major engineering effort and domain expertise. Examples of such services include anomaly detection, adaptive recommender systems, time series forecasting, and real-time monitoring. More generally, the requirements these services specifically pose are that they must operate continuously with tolerance to failure, adaptively with respect to concept drift and resource changes, and flexibly by incorporating business logic.

% To this end, data analysts resort to system-frameworks for data intensive computing which provide a means for writing applications oriented around specific abstract data types and operations. By being abstract, analysts are able to ask questions about data to a system without needing to know the how the system arrives at its conclusions. Modern representative examples of such frameworks include:

% \begin{itemize}
%     \item \textbf{TensorFlow} \cite{TensorFlow}, a framework for portable machine learning oriented around tensors and linear algebra.
%     \item \textbf{Apache Flink} \cite{Flink}, a framework for low-latency stream processing oriented around data streams and stateful event-based logic.
%     \item \textbf{Apache Spark} \cite{Spark}, a framework for high-throughput batch processing oriented around data frames and relational algebra.
%     \item \textbf{Apache Giraph} \cite{Giraph}, a framework for large scale graph processing oriented around vertex-centric computation.
%     \item \textbf{Ray RLib} \cite{Ray}, a framework for distributed reinforcement learning oriented around agents, environments, and policies.
% \end{itemize}
% 
% A typical end-to-end deep analytics pipeline combines traditional data processing stages with machine learning and therefore requires a system and framework that supports relational algebra, graph processing, and linear algebra components. There is no doubt a combination of the aforementioned frameworks could be used for such analytics. However, as of yet there exists no framework and system that bridges each of the programming models and workloads while also supporting the real-time aspect of live data. This problem forces developers to deal with two types of complexity. First, they have to learn how to use and integrate multiple frameworks which in the worst case are written in different programming languages. Second, they have to consider low-level details that concern how systems interact, such as state management and data transfer, to achieve good performance.
% 
% In the CDA group, we are addressing this problem by engineering a whole new infrastructure for Continuous Deep Analytics. This infrastructure includes 1) a middleware for building distributed systems\footnote{Website: https://github.com/kompics/kompact}, 2) a distributed system built on top of the middleware for scalable stream and batch processing \citeP{Arcon} \footnote{Website: https://github.com/cda-group/arcon}, and 3) a domain-specific language for data science integrated to the system for programming with multiple abstract data types \citeP{Arc} \footnote{Website: https://github.com/cda-group/arc}. CDA is an interdisciplinary project funded by SSF, driven by a world-leading team of researchers at RISE and KTH, and destined to finish in the summer of 2023. The project in addition includes research on the development of novel online machine learning algorithms to be deployed on the CDA infrastructure. Project goals and preliminary results can be read more about in our midterm report \cite{CDA-Midterm}.

% * What is the problem?
% * What is the vision? (ideal solution)
% * What is our approach? (The CDA system: Applications, Arc-Lang, Arc-MLIR, Arcon, Kompact)
% * What are the challenges (requirements)?

\section{Introduction to Arc-Lang}

% * Research Questions and Hypotheses
% * Background + Related Work (Existing solutions)
% * Problem and Motivation (Why Arc-Lang?)
% * Applications (What can Arc-Lang do / not do?)
% * Design principles

\subsection{Tour of Arc-Lang}

% * Unique features (What makes Arc-Lang special?)
% * Common features (What does Arc-Lang borrow from other solutions?)
% * Example programs (How can you use the features to solve problems?)

\subsection{High-Level Model}

% Describe the language: Syntax and semantics

\subsection{Execution Model}

% Describe the runtime
% * Channels
% * Tasks
% * Data structures
